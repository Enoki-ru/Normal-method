{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a32051c2-2844-43c1-b654-0f9a5380d31c",
   "metadata": {},
   "source": [
    "# \"Нормальный\" метод, или\n",
    "# Наивный баесовский классификатор\n",
    "Привет, сегодня мы будем изучать такую вот... вещь. Мда уж, звучит не очень, но сегодня мы точно разберемся, почему его называют наивным, почему он баесовский, и почему вообще классификатор. Поехали!\n",
    "1. Почему Наивный\n",
    "Дело в том, что для реализации алгоритма мы принимаем так называемое *допущение,* что каждый исследуемый нами параметр данных не зависит от других параметров, то есть они не оказывают никакое влияние друг на друга. К примеру, рост и вес человека имеют некое влияние друг на друга, ведь верно будет предположить, что более высокий человек в среднем будет весить больше других.\n",
    "## Пример\n",
    "Давайте сейчас быстренько посмотрим на примере, как работает такого рода алгоритм. Для начала, давайте создадим набор данных Fruits из магазина какого-нибудь дяди Пети на рынке. В них мы представим данные о различных видах фруктов: бананах, апельсинах, и сливах. Сделаем для них несколько факторов, таких, как, к примеру:\n",
    "- Кол-во Длинных (Long)\n",
    "- Кол-во Сладких (Sweet)\n",
    "- Кол-во Желтых (yellow)\n",
    "- Количество в магазине (Total)\n",
    "Создадим набор таких данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68f5bbd9-fe15-4bcb-8774-15e283ef9d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Long</th>\n",
       "      <th>Sweet</th>\n",
       "      <th>Yellow</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Banana</th>\n",
       "      <td>400</td>\n",
       "      <td>350</td>\n",
       "      <td>450</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orange</th>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Plum</th>\n",
       "      <td>30</td>\n",
       "      <td>180</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>430</td>\n",
       "      <td>680</td>\n",
       "      <td>850</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Long  Sweet  Yellow  Total\n",
       "Banana   400    350     450    500\n",
       "Orange     0    150     300    300\n",
       "Plum      30    180     100    200\n",
       "Total    430    680     850   1000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = np.array([[400, 350, 450, 500],\n",
    "                 [0, 150, 300, 300],\n",
    "                 [30, 180, 100, 200],\n",
    "                 [430, 680, 850, 1000]])\n",
    "\n",
    "idx = ['Banana', 'Orange', 'Plum', 'Total']\n",
    "col = ['Long', 'Sweet', 'Yellow', 'Total']\n",
    "\n",
    "fruits = pd.DataFrame(data, columns=col, index=idx)\n",
    "fruits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a08733-8b6f-45df-a16b-df50327ddc05",
   "metadata": {},
   "source": [
    "Данный метод на самом деле оперирует теорией вероятностей, значит, к примеру, если мы возьмем 1 банан среди этих 500-ста, то с вероятностью 80% он будет длинный, с вероятностью 70% он будет сладким, и 90% уйдет на то что он будет желтым.\n",
    "Допустим, мы хотим узнать, к какому классу относится фрукт с параметрами {длинный, сладкий, желтый}. Вероятность того, что объект принадлежит какому-либо классу при данных параметрах обозначается как: \n",
    "\n",
    "$$\n",
    "P(Class|Long, Sweet, Yellow)\n",
    "$$\n",
    "\n",
    "Чтобы вычислить вероятность в случае дискретных (конечных) данных, мы делим количество соответствующих признаку объектов на общее число объектов. Например, если мы хотим узнать вероятность $P(Long|Banana)$, мы вычисляем $400 / 500 = 80$%, как уже упоминалось выше.\n",
    "Данный метод основан на теореме английского математика-статистика Томаса Байеса. По сути, она позволяет предсказать класс на основании набора параметров, используя вероятность. Общая формула Байеса для класса с одним признаком выглядит так:\n",
    "\n",
    "$$\n",
    "P(Class A|Feature 1) = \\frac{P(Feature 1|Class A)\\cdot P(Class A)}{P(Feature 1)}\n",
    "$$\n",
    "\n",
    "$P(Class A|Feature 1)$ - вероятность того, что объект является классом $A$ при том, что его признак соответствует $Feature 1$.\n",
    "\n",
    "Упрощенное уравнение для классификации при двух признаках выглядит так:\n",
    "\n",
    "$$\n",
    "P(Class A|Feature 1, Feature 2) = \\frac{P(Feature 1|Class A)\\cdot P(Feature 2|Class A)\\cdot P(Class A)}{P(Feature 1)\\cdot P(Feature 2)}\n",
    "$$\n",
    "\n",
    "И далее для большего количества признаков формула меняется соответствующим образом.\n",
    "\n",
    "Напомню глупеньким (кек в), что вероятность - число, которое принимает значения от 0 до 1, при этом 0 - полное несоответствие класса признакам, а 1 - однозначно определенный класс. Соответственно, чем ближе значение вероятности определенного класса к 1, тем больше шанс того, что объект принадлежит именно этому классу.\n",
    "Знаменатель можно проигнорировать, так как он будет одинаков для всех вычислений и никакой важной информации не даст.\n",
    "Тогда получится следующее уравнение:\n",
    "\n",
    "$$\n",
    "P(Class A|Feature 1, Feature 2) = P(Feature 1|Class A)\\cdot P(Feature 2|Class A)\\cdot P(Class A)\n",
    "$$\n",
    "\n",
    "То есть для каждого возможного класса вычисляем только произведение вероятностей того, что каждый признак соответствует классу, и вероятности того, что объект принадлежит этому классу. Соответственно, наибольшее значение этого произведения, рассчитанного с признаками конкретного объекта, для какого-то из классов будет указывать на принадлежность объекта к этому классу. <br>\n",
    "Вернемся к нашему примеру с фруктами. Чтобы понять, к какому классу принадлежит объект с признаками {Long, Sweet, Yellow}, рассчитаем для каждого из классов формулу Байеса, используя для этого данные частотной таблицы. На выходе получаем вероятность того, что объект принадлежит классу. Класс с наибольшей вероятностью является ответом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed272594-db68-4a93-8fb1-eaa5370170eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 400  350  450  500]\n",
      " [   0  150  300  300]\n",
      " [  30  180  100  200]\n",
      " [ 430  680  850 1000]]\n",
      "Сначала p=1\n",
      "p = p * 400/500 = 0.8\n",
      "p = p * 350/500 = 0.5599999999999999\n",
      "p = p * 450/500 = 0.504\n",
      "Сначала p=1\n",
      "p = p * 0/300 = 0.0\n",
      "p = p * 150/300 = 0.0\n",
      "p = p * 300/300 = 0.0\n",
      "Сначала p=1\n",
      "p = p * 30/200 = 0.15\n",
      "p = p * 180/200 = 0.135\n",
      "p = p * 100/200 = 0.0675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Banana': 0.252, 'Orange': 0.0, 'Plum': 0.013500000000000002}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = {}\n",
    "\n",
    "print(fruits.values)\n",
    "\n",
    "for i in range (fruits.values.shape[0] - 1):\n",
    "    p = 1\n",
    "    \n",
    "    print(f\"Сначала p={p}\")\n",
    "    \n",
    "    for j in range (fruits.values.shape[1] - 1):\n",
    "        p *= fruits.values[i, j] / fruits.values[i, -1]\n",
    "        \n",
    "        print(f\"p = p * {fruits.values[i, j]}/{fruits.values[i, -1]} = {p}\")\n",
    "        \n",
    "    p *= fruits.values[i, -1] / fruits.values[-1, -1]\n",
    "    \n",
    "    result[fruits.index[i]] = p\n",
    "    \n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9497d9-0df1-4a8b-85d9-6bf3fac60bbe",
   "metadata": {},
   "source": [
    "Как видите, мы перемножили вероятностей всех классов для каждого из фрукта, и запихнули в отдельный кортеж (вроде бы так называется). <br>\n",
    "Ну, и как вы можете заметить, вероятность того, что у фрукта одновременность будет и длина, и желтость, и сочность, будет у банана! 25%! \n",
    "\n",
    "Но что делать, если у нас не частотная таблица, как в примере выше, а непрерывные данные? Например, мы не можем вычислять, сколько человек с конкретным ростом 1,81м, 1,67м и т.д. присутствует в выборке - нам это попросту ничего не даст, а лишь добавит громоздких вычислений. Поэтому обычно при непрерывных значениях параметров используется гауссовский наивный Байес, в котором сделано предположение о том, что значения параметров взяты из нормального распределения.\n",
    "![image.png](GNB.png)\n",
    "\n",
    "Сори, но дальше возьму уже тупо готовую теорию, потому что я это идельано знаю и помню.\n",
    "\n",
    "На графике - плотность вероятности нормального распределения. По сути, где больше площадь под графиком, там и наиболее вероятные значения. Поскольку способ представления значений в наборе данных изменяется, то и формула условной вероятности изменяется на \n",
    "\n",
    "$$\n",
    "p(x_i|y) = \\frac{1}{\\sqrt{2\\pi \\sigma^2_y}}exp(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y})\n",
    "$$\n",
    "\n",
    "Здесь $\\sigma^2$ - дисперсия (разброс) данных, а $\\mu$ - математическое ожидание (среднее значение). При этом $y$ - предполагаемый класс, а $x_i$ - значение признака у того объекта, который нужно классифицировать.\n",
    "\n",
    "По большому счету в нашей начальной формуле для вычисления вероятности того, что объект с данными признаками относится к конкретному классу, мы просто заменяем формулу вычисления вероятности как отношения количества соответствующих признаку объектов к общему числу объектов на данную, а дальше проводим идентичные вычисления.\n",
    "\n",
    "Теперь попробуем использовать Гауссову кривую, построенную по нашим дискретным значениям, чтобы вывести вероятность на новый уровень, способную находить вероятность для любого события EVER!\n",
    "\n",
    "Теперь импортируем модель GaussianNB из библиотеки sklearn и посмотрим, как она работает на уже известном нам датасете Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5dde43d8-3f0b-483f-9f40-b41de880e7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_dataset=load_iris() # возвращает объект с несколькими полями\n",
    "\n",
    "print(iris_dataset.keys())\n",
    "\n",
    "iris_dataframe=pd.DataFrame(iris_dataset['data'], columns = iris_dataset.feature_names)\n",
    "iris_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e29fda15-3d37-4423-8e9d-96c3d7dbdc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (112, 2), y_train shape: (112,),\n",
      "X_test shape: (38, 2), y_test shape: (38,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris_dataset.data[:, 2:4], \n",
    "                                                    iris_dataset['target'],\n",
    "                                                    random_state=57) # random_state - для воспроизводимости\n",
    "\n",
    "print(f'X_train shape: {x_train.shape}, y_train shape: {y_train.shape},\\n'\n",
    "      f'X_test shape: {x_test.shape}, y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d57534d9-dcc3-42ab-8364-cc81ee998564",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = nb.fit(x_train, y_train) # Если возникают вопросы, что, зачем, и почему, возвращайтесь на предыдущую статью, вам здесь пока рано находиться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "646cf24c-b177-4144-aafc-5bccac2b3906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 1, 0, 1, 0, 2, 1, 0, 1, 0, 0, 0, 1, 2, 1, 0, 0, 2, 2, 0,\n",
       "       2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 0, 0, 1, 2, 0, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_predictions=nb.predict(x_test)\n",
    "nb_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4f78f8-d53d-41a9-b1a7-9c2b132c2d23",
   "metadata": {},
   "source": [
    "Для определения точности предсказаний воспользуемся встроенной функцией *score*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a45a310b-14e7-41f6-9c1b-158ed5d551e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy= nb.score(x_test, y_test)\n",
    "print(f\"Accuracy:{accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97be48e-f1f2-4206-a42f-3e6b9825430e",
   "metadata": {},
   "source": [
    "В итоге мы реализовали \"Нормальный\" метод, будем его так теперь)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
